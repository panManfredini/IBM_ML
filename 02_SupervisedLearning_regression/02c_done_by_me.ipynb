{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Section 2, Part c: Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "* Chain multiple data processing steps together using `Pipeline`\n",
    "* Use the `KFolds` object to split data into multiple folds.\n",
    "* Perform cross validation using SciKit Learn with `cross_val_predict` and `GridSearchCV`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pickle.load(open(\"boston_housing_clean.pickle\",mode=\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataframe', 'description'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = boston['dataframe']\n",
    "description = boston['description']\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(shuffle=True, random_state=23456, n_splits=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting in x and Y\n",
    "x = df.drop(columns=[\"MEDV\"])\n",
    "y = df[\"MEDV\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6315392061287662, 0.7556727904163659, 0.7130344151773451]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for train_idx, test_idx in kf.split(x):\n",
    "    temp_train_feat = x.iloc[train_idx]\n",
    "    temp_train_targ = y[train_idx]\n",
    "    temp_test_feat  = x.iloc[test_idx]\n",
    "    temp_test_targ  = y[test_idx]\n",
    "    lr.fit(temp_train_feat,temp_train_targ)\n",
    "    temp_score = r2_score(temp_test_targ, lr.predict(temp_test_feat))\n",
    "    scores.append(temp_score)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: \n",
    "\n",
    "Suppose we want to do Linear Regression on our dataset to get an estimate, based on mean squared error, of how well our model will perform on data outside our dataset. \n",
    "\n",
    "Suppose also that our data is split into three folds: Fold 1, Fold 2, and Fold 3.\n",
    "\n",
    "What would the steps be, in English, to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your response below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding this up\n",
    "\n",
    "The [`KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) object in SciKit Learn tells the cross validation object (see below) how to split up the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit cumbersome, but do-able."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion (Part 2): \n",
    "\n",
    "Now suppose we want to do the same, but appropriately scaling our data as we go through the folds.\n",
    "\n",
    "What would the steps be _now_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your response below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding this up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T17:32:09.978972Z",
     "start_time": "2019-02-19T17:32:09.974341Z"
    }
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(same scores, because for vanilla linear regression with no regularization, scaling actually doesn't matter for performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting quite cumbersome! \n",
    "\n",
    "_Very_ luckily, SciKit Learn has some wonderful functions that handle a lot of this for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Pipeline` and `cross_val_predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline` lets you chain together multiple operators on your data that both have a `fit` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple processing steps into a `Pipeline`\n",
    "\n",
    "A pipeline contains a series of steps, where a step is (\"name of step\", actual_model). The \"name of step\" string is only used to help you identify which step you are on, and to allow you to specify parameters at that step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "model = Pipeline([(\"scale\", scaler),(\"regression\", lr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.709355597323319"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cross_val_predict(model, x,y, cv=kf)\n",
    "r2_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cross_val_predict`\n",
    "\n",
    "[`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) is a function that does K-fold cross validation for us, appropriately fitting and transforming at every step of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `cross_val_predict` doesn't use the same model for all steps; the predictions for each row are made when that row is in the validation set. We really have the collected results of 3 (i.e. `kf.num_splits`) different models. \n",
    "\n",
    "When we are done, `estimator` is still not fitted. If we want to predict on _new_ data, we still have to train our `estimator`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "**Hyperparameter tuning** involves using cross validation (or train-test split) to determine which hyperparameters are most likely to generate a model that _generalizes_ well outside of your sample.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "We can generate an exponentially spaces range of values using the numpy [`geomspace`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html#numpy.geomspace) function.\n",
    "\n",
    "```python\n",
    "np.geomspace(1, 1000, num=4)\n",
    "```\n",
    "\n",
    "produces:\n",
    "\n",
    "```\n",
    "array([    1.,    10.,   100.,  1000.])\n",
    "```\n",
    "\n",
    "Use this function to generate a list of length 10 called `alphas` for hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tunes the `alpha` hyperparameter for Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.geomspace(1E-9, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7093555973625159, 0.7093555977137371, 0.7093556012543076, 0.7093556363286851, 0.7093559910357743, 0.7093594780985493, 0.7093942707111385, 0.7096691684984482, 0.7059294806227676, 0.6497524048292167]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for a in alpha:\n",
    "    las = Lasso(a)\n",
    "    est = Pipeline([(\"scale\",scaler),(\"regress\",las)])\n",
    "    predictions = cross_val_predict(est,x,y,cv=kf)\n",
    "    scores.append(r2_score(y,predictions))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Add `PolynomialFeatures` to this `Pipeline`, and re-run the cross validation with the `PolynomialFeatures` added.\n",
    "\n",
    "**Hint #1:** pipelines process input from first to last. Think about the order that it would make sense to add Polynomial Features to the data in sequence and add them in the appropriate place in the pipeline.\n",
    "\n",
    "**Hint #2:** you should see a significant increase in cross validation accuracy from doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8243791979141576, 0.8208890182039438, 0.8165139975565484, 0.8106795759483365, 0.8052684463213485, 0.7985329609276103, 0.7902070430889646, 0.7840762184090483, 0.7781060983455144, 0.7695807498503155, 0.7571273164061386, 0.7424811804193143, 0.7254114866956423, 0.7076424462723467, 0.6863516173813768, 0.6514446979166446, 0.5956123979745542, 0.5074947006831048, 0.3675786369581394, 0.1386769665159696]\n"
     ]
    }
   ],
   "source": [
    "pol = PolynomialFeatures(degree=3)\n",
    "scores = []\n",
    "alphas = np.geomspace(0.06, 6.0, 20)\n",
    "for a in alphas:\n",
    "    las = Lasso(a, max_iter=100000)\n",
    "    est = Pipeline([(\"pol\",pol),(\"scale\",scaler),(\"regress\",las)])\n",
    "    predictions = cross_val_predict(est,x,y,cv=kf)\n",
    "    scores.append(r2_score(y,predictions))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you store the results in a list called `scores`, the following will work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Do the same, but with `Ridge` regression \n",
    "\n",
    "Which model, `Ridge` or `Lasso`, performs best with its optimal hyperparameters on the Boston dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Both Lasso and Ridge with proper hyperparameter tuning give better results than plain ol' Linear Regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f64bb43fd00>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pol = PolynomialFeatures(degree=3)\n",
    "scores = []\n",
    "alphas = np.geomspace(0.06, 6.0, 20)\n",
    "for a in alphas:\n",
    "    ridge = Ridge(a, max_iter=100000)\n",
    "    est = Pipeline([(\"pol\",pol),(\"scale\",scaler),(\"regress\",ridge)])\n",
    "    predictions = cross_val_predict(est,x,y,cv=kf)\n",
    "    scores.append(r2_score(y,predictions))\n",
    "plt.plot(alphas, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = PolynomialFeatures(degree=2)\n",
    "ridge = Ridge(5,max_iter=100000)\n",
    "est = Pipeline([(\"pol\",pol),(\"scale\",scaler),(\"regress\",ridge)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRIM</td>\n",
       "      <td>0.224158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZN</td>\n",
       "      <td>-1.014847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INDUS</td>\n",
       "      <td>1.066566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHAS</td>\n",
       "      <td>0.866939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>PTRATIO B</td>\n",
       "      <td>0.199198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>PTRATIO LSTAT</td>\n",
       "      <td>0.184028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>B^2</td>\n",
       "      <td>-0.663494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>B LSTAT</td>\n",
       "      <td>-1.393840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>LSTAT^2</td>\n",
       "      <td>4.539291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1\n",
       "0                1  0.000000\n",
       "1             CRIM  0.224158\n",
       "2               ZN -1.014847\n",
       "3            INDUS  1.066566\n",
       "4             CHAS  0.866939\n",
       "..             ...       ...\n",
       "100      PTRATIO B  0.199198\n",
       "101  PTRATIO LSTAT  0.184028\n",
       "102            B^2 -0.663494\n",
       "103        B LSTAT -1.393840\n",
       "104        LSTAT^2  4.539291\n",
       "\n",
       "[105 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.fit(x,y)\n",
    "pd.DataFrame(zip(est.named_steps['pol'].get_feature_names(input_features=list(x.columns)), \n",
    "    est.named_steps['regress'].coef_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for whatever your best overall hyperparameter was: \n",
    "\n",
    "* Standardize the data\n",
    "* Fit and predict on the entire dataset\n",
    "* See what the largest coefficients were\n",
    "    * Hint: use \n",
    "    ```python\n",
    "    dict(zip(model.coef_, pf.get_feature_names()))\n",
    "    ```\n",
    "    for your model `model` to get the feature names from `PolynomialFeatures`.\n",
    "    \n",
    "    Then, use\n",
    "    ```python\n",
    "    dict(zip(list(range(len(X.columns.values))), X.columns.values))\n",
    "    ```\n",
    "    \n",
    "    to see which features in the `PolynomialFeatures` DataFrame correspond to which columns in the original DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do cross-validation, we used two techniques:\n",
    "- use `KFolds` and manually create a loop to do cross-validation\n",
    "- use `cross_val_predict` and `score` to get a cross-valiated score in a couple of lines.\n",
    "\n",
    "To do hyper-parameter tuning, we see a general pattern:\n",
    "- use `cross_val_predict` and `score` in a manually written loop over hyperparemeters, then select the best one.\n",
    "\n",
    "Perhaps not surprisingly, there is a function that does this for us -- `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T18:02:49.319148Z",
     "start_time": "2019-02-19T18:02:46.093880Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T18:03:07.016198Z",
     "start_time": "2019-02-19T18:03:07.010215Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T18:03:31.735568Z",
     "start_time": "2019-02-19T18:03:31.728658Z"
    }
   },
   "outputs": [],
   "source": [
    "y_predict = grid.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T18:04:17.838943Z",
     "start_time": "2019-02-19T18:04:17.832872Z"
    }
   },
   "outputs": [],
   "source": [
    "# This includes both in-sample and out-of-sample\n",
    "r2_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T18:04:27.088854Z",
     "start_time": "2019-02-19T18:04:27.082915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notice that \"grid\" is a fit object!\n",
    "# We can use grid.predict(X_test) to get brand new predictions!\n",
    "grid.best_estimator_.named_steps['ridge_regression'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T18:05:34.756588Z",
     "start_time": "2019-02-19T18:05:34.728508Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. We can manually generate folds by using `KFolds`\n",
    "2. We can get a score using `cross_val_predict(X, y, cv=KFoldObject_or_integer)`. \n",
    "   This will produce the out-of-bag prediction for each row.\n",
    "3. When doing hyperparameter selection, we should be optimizing on out-of-bag scores. This means either using `cross_val_predict` in a loop, or ....\n",
    "4. .... use `GridSearchCV`. GridSearchCV takes a model (or pipeline) and a dictionary of parameters to scan over. It finds the hyperparameter set that has the best out-of-sample score on all the parameters, and calls that it's \"best estimator\". It then retrains on all data with the \"best\" hyper-parameters.\n",
    "\n",
    "### Extensions\n",
    "\n",
    "Here are some additional items to keep in mind:\n",
    "* There is a `RandomSearchCV` that tries random combination of model parameters. This can be helpful if you have a prohibitive number of combinations to test them all exhaustively.\n",
    "* KFolds will randomly select rows to be in the training and test folds. There are other methods (such as `StratifiedKFolds` and `GroupKFold`, which are useful when you need more control over how the data is split (e.g. to prevent data leakage). You can create these specialized objects and pass them to the `cv` argument of `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
